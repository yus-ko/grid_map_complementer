{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33703fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class GridMapDataset(Dataset):\n",
    "    def __init__(self, root, split='train', attr_name=None, transform=None):\n",
    "        root_dir = Path(root)\n",
    "        self.image_files = [f for f in root_dir.iterdir() if f.is_file()]\n",
    "\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((128,128)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_file = self.image_files[idx]\n",
    "        img = Image.open(img_file).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "\n",
    "        return {'data': img}\n",
    "\n",
    "# 使用例: DataLoader を作ってバッチサイズと形状を確認\n",
    "try:\n",
    "    dataset = GridMapDataset(\"/path/to/dir\")\n",
    "    from torch.utils.data import DataLoader\n",
    "    loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "    batch = next(iter(loader))\n",
    "    print('data.shape =', batch['data'].shape)\n",
    "except Exception as e:\n",
    "    print('Error creating/using CelebADaIllustDatasettaset:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63e3af4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# マスクを作るユーティリティと可視化関数\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import tqdm\n",
    "\n",
    "def random_mask(shape, mask_size=(32,32)):\n",
    "    \"\"\"与えられた (C,H,W) tensor のランダムな矩形を 0 にするマスクを返す\"\"\"\n",
    "    C, H, W = shape\n",
    "    mh, mw = mask_size\n",
    "    top = random.randint(0, H - mh)\n",
    "    left = random.randint(0, W - mw)\n",
    "    mask = torch.ones((C, H, W), dtype=torch.float32)\n",
    "    mask[:, top:top+mh, left:left+mw] = 0.0\n",
    "    return mask\n",
    "\n",
    "\n",
    "def show_masked(image_tensor, mask, title=None):\n",
    "    # image_tensor: C,H,W\n",
    "    im = image_tensor.clone()\n",
    "    im = im * mask\n",
    "    im = im.permute(1,2,0).numpy()\n",
    "    im = (im * 0.5) + 0.5  # unnormalize\n",
    "    plt.figure(figsize=(3,3))\n",
    "    plt.imshow(im)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.axis('off')\n",
    "\n",
    "\n",
    "def save_im(image_tensor, mask, name=\"masked.jpg\"):\n",
    "    \"\"\"image_tensor (C,H,W) と mask (C,H,W) を乗算した結果（im）を画像ファイルとして保存する\"\"\"\n",
    "    try:\n",
    "        im = image_tensor.clone()\n",
    "        im = im * mask\n",
    "        # C,H,W -> H,W,C, convert to CPU numpy\n",
    "        im_np = im.permute(1,2,0).cpu().numpy()\n",
    "        # unnormalize range -1..1 -> 0..1  あるいは 0..1 の場合もあるので安全に扱う\n",
    "        im_np = (im_np * 0.5) + 0.5\n",
    "        im_uint8 = (im_np * 255).clip(0,255).astype('uint8')\n",
    "        # PIL expects HxW or HxWx3\n",
    "        if im_uint8.shape[2] == 1:\n",
    "            img_pil = Image.fromarray(im_uint8[:,:,0], mode='L')\n",
    "        else:\n",
    "            img_pil = Image.fromarray(im_uint8)\n",
    "        path = Path.cwd() / name\n",
    "        img_pil.save(str(path))\n",
    "        print(f'Saved masked image to: {path}')\n",
    "    except Exception as e:\n",
    "        print('Failed to save masked image:', e)\n",
    "\n",
    "# for i in tqdm.tqdm(range(len(dataset))):\n",
    "# for i in range(len(dataset)):\n",
    "#     sample = dataset[i]['data']\n",
    "#     m = random_mask(sample.shape, mask_size=(32,32))\n",
    "#     save_im(sample, m, f'dataset/celeba/mask_32_32/{i:06}.jpg')\n",
    "\n",
    "# 簡単なテスト（dataset が使えるなら）。生成した im を画像として保存する\n",
    "try:\n",
    "    num = random.randint(0, len(dataset)-1)\n",
    "    sample = dataset[num]['data']\n",
    "    m = random_mask(sample.shape, mask_size=(32,32))\n",
    "    show_masked(sample, m, f'Masked sample {num}')\n",
    "except Exception as e:\n",
    "    print('Mask util test skipped:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43123dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# シンプルな畳み込みオートエンコーダと学習ループ\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "\n",
    "class SimpleAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 3, 4, stride=2, padding=1),\n",
    "            nn.Tanh()  # 出力は正規化範囲に合わせて -1..1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        out = self.decoder(z)\n",
    "        return out\n",
    "\n",
    "try:\n",
    "    checkpoint_path = \"grid_map_complement.pt\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = SimpleAutoEncoder().to(device)\n",
    "    optim = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    early_stopping_best_val_loss = float(\"inf\")\n",
    "    epochs = 10000\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        # for batch in tqdm.tqdm(loader):\n",
    "        for batch in loader:\n",
    "            imgs = batch['data'].to(device)\n",
    "            masks = torch.stack([random_mask(img.shape, mask_size=(32,32)) for img in imgs]).to(device)\n",
    "            masked = imgs * masks\n",
    "\n",
    "            recon = model(masked)\n",
    "            loss = criterion(recon * (1-masks), imgs * (1-masks))  # マスク領域の損失を最小化\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs} loss={total_loss/len(loader):.6f}\")\n",
    "\n",
    "        # モデルのセーブ\n",
    "        if total_loss < early_stopping_best_val_loss:\n",
    "            early_stopping_best_val_loss = total_loss\n",
    "            early_stopping_patience_counter = 0\n",
    "            # ベストなモデルとして Checkpoint を更新する\n",
    "            checkpoint_params = {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optim.state_dict(),\n",
    "                \"loss\": total_loss,\n",
    "            }\n",
    "            torch.save(\n",
    "                checkpoint_params,\n",
    "                checkpoint_path,\n",
    "            )\n",
    "\n",
    "except Exception as e:\n",
    "    print('Training/test loop skipped or failed:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a4eaeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 バッチを可視化して補間結果を確認（マスク領域を pred で置き換える）\n",
    "batch = next(iter(loader))\n",
    "imgs = batch['data']\n",
    "masks = torch.stack([random_mask(img.shape, mask_size=(32,32)) for img in imgs])\n",
    "masked = imgs * masks\n",
    "\n",
    "# モデルのロード\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(masked.to(device)).cpu()\n",
    "\n",
    "# マスク領域を pred で置き換え: masks=1 が保持領域、0 が補間領域と想定\n",
    "# ここでは masks が 1:保持, 0:穴 の形式なので、補完画像は imgs * masks + pred * (1-masks)\n",
    "filled = imgs * masks + pred * (1 - masks)\n",
    "\n",
    "# 最初の画像を表示: 原画 / マスク / 補間 / マスクで埋めた結果\n",
    "show_masked(imgs[0], torch.ones_like(imgs[0]), 'Original')\n",
    "show_masked(imgs[0], masks[0], 'Masked')\n",
    "show_masked(pred[0], torch.ones_like(pred[0]), 'Reconstructed')\n",
    "show_masked(filled[0], torch.ones_like(filled[0]), 'Filled (masked regions replaced by Reconstructed)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57f9e5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単独で実行する場合に filled を表示\n",
    "try:\n",
    "    show_masked(filled[0], torch.ones_like(filled[0]), 'Filled (masked regions replaced by pred)')\n",
    "except NameError:\n",
    "    print('Variable filled not found. Run the previous cell first.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
